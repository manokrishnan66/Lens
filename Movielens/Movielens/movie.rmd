---
title: "HarvardX: PH125.9x Data Science  \n   MovieLens Rating Prediction Project"
author: "Mano Krishnan"
date: "March 09, 2020"
output:
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
---
\pagebreak

# Introduction

The main objective here is to produce product recommendations of an analytical system by applying statistical techniques. We are taking 'movielens' database to predict ratings of the movies.

The 10M version of the dataset is available in the grouplens website. Based on the different statistical models, we will build a rating predictor system. 

### Dataset and Data Loading
- [MovieLens 10M] https://grouplens.org/datasets/movielens/10m/
- [MovieLens 10M- zip file] http://files.grouplens.org/datasets/movielens/ml-10m.zip

Data Loading 

```{r data_load, warning=FALSE, error=FALSE, message=FALSE}
tinytex::install_tinytex()
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

library(readr)
movies <- read_delim("ml-10M100K/movies.dat", 
                     "::", escape_double = FALSE, col_names = FALSE, 
                     trim_ws = TRUE)
View(movies)
ratings <- read_delim("ml-10M100K/ratings.dat", 
                     "::", escape_double = FALSE, col_names = FALSE, 
                     trim_ws = TRUE)


movies <- movies %>% select(-X2,-X4)
colnames(movies) <- c("movieId", "title", "genres")

ratings <- ratings %>% select(-X2,-X4,-X6)
colnames(ratings) <- c("userId", "movieId", "rating", "timestamp")

movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId)[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))

ratings <- as.data.frame(ratings) %>% mutate(userId = as.numeric(userId),
                                              movieId = as.numeric(movieId),
                                              rating = as.numeric(rating),
                                             timestamp = as.numeric(timestamp))


movielens <- left_join(ratings, movies, by = "movieId")


```

### Libraries

The following libraries were used in this report:
```{r libs, warning=FALSE, error=FALSE, message=FALSE}
library(ggplot2)
library(lubridate)
library(caret)
library(tidyverse)
```

### Aim & Objectives

The provided dataset is divided into training set and validation set. We are training the first set with the machine learning algorithms and to predict movie ratings in the validation set.

Data visualization and data exploration is used to find the interesting trends and the factors affecting the users' ratings. We are creating four models based on their resulting RMSE and finalizing the optimal model to predict the movie ratings.  

# Methodology & Analysis
## Data Pre-processing
### Evaluation of Predicted Ratings using RMSE

The root-mean-square deviation (RMSD) or root-mean-square error (RMSE) is a frequently used measure of the differences between values (sample or population values) predicted by a model or an estimator and the values observed. Here, we are using the RMSE value to evaluate each model.

The function that computes the RMSE for vectors of ratings and their corresponding predictors will be the following:
$$ RMSE = \sqrt{\frac{1}{N}\displaystyle\sum_{u,i} (\hat{y}_{u,i}-y_{u,i})^{2}} $$


```{r RMSE function}
# function to calculate the RMSE values
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2,na.rm = T))
}
```

### Split Raw Data: Train and Test Sets

We can partition the movielens dataset into 2 sets. One set is used for building the algorithm and the second set are used for the validation of the model. The 10% of the movielens data represents the validation set.

```{r Split_data}
set.seed(1)
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")
# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)
validation_CM <- validation  
validation <- validation %>% select(-rating)
```

### Modifying the Year

The title column is merged with name of the movie and the year of release. So, we are splitting the title column into name and the year column. So that we can find the dependencies between years of release and rating.

```{r modifying year}
# Modify the year as a column in the edx & validation datasets
edx <- edx%>%separate(title,c("name", "year"), "\\s*\\((?=\\d+\\)$)|\\)$")
  validation <- validation%>%separate(title,c("name", "year"), "\\s*\\((?=\\d+\\)$)|\\)$")
```


## Data Visualization and Data Exploration

### General Data Information
```{r data_info}
# The 1st rows of the edx are presented below:
head(edx) 

# Summary Statistics of edx
summary(edx)

# Number of unique movies and users in the edx dataset 
edx %>% summarize(n_users = n_distinct(userId), n_movies = n_distinct(movieId))
```

### Distribution of Ratings

Most common ratings are 3 and 4 compared to other ratings. Then half star ratings are less popular to whole star ratings. The preference of the users is with the higher ratings than lower ratings.


```{r distribution of ratings}
 edx %>% 
    ggplot(aes(rating)) + geom_histogram(binwidth=0.25, fill = "red") +
    scale_x_discrete(limits = c(seq(0,5,1))) +
    scale_y_continuous(breaks = c(seq(0, 3000000, 250000))) +
    ggtitle("Distribution of Rating")
```

### Ratings per movie

The majority of the movies have been rated between 50 and 1000 times. Another interesting fact shows that around 125 movies have been rated only once. These scenarios pushed us to add a penalty term in the model preparation.

```{r ratings_movies}
edx %>%
    count(movieId) %>%
    ggplot(aes(n)) +
    geom_histogram(bins = 30, color = "maroon", fill = "green") +
    scale_x_log10() +
  geom_hline(yintercept=125, color = "brown",size=1) +
    xlab("No. of ratings") +
    ylab("No.of movies") +
    ggtitle("No. ratings per movie")

```

### Number of ratings by Number of Users

The majority people have rated below 100 movies and above 30 movies. So a penalty term would be added for this.


```{r users_ratings}  
edx %>%
    count(userId) %>%
    ggplot(aes(n)) +
    geom_histogram(bins = 30, color = "white", fill = "maroon") +
    scale_x_log10() +
    xlab("Number of ratings") + 
    ylab("Number of users") +
    ggtitle("Number of ratings given by users")
  
```
### Total movie ratings per genre

This shows the genre details of the movielens dataset.

```{r count_genres}  
edx%>%
    group_by(genres) %>%
    summarize(count = n()) %>%
    arrange(desc(count))
```

### Mean movie ratings by users

```{r mean_movie_ratings}
edx %>%
group_by(userId) %>%
filter(n() >= 100) %>%
summarize(b = mean(rating)) %>%
ggplot(aes(b)) +
geom_histogram(bins = 30, fill = "blue", color = "black") +
xlab("Mean-rating") +
ylab("No of users") +
ggtitle("Mean movie ratings by users") +
scale_x_discrete(limits = c(seq(0,5,1))) +
theme_light()

```


## Data Analysis and modelling

```{r rmse_results_initiation}
#Initiate RMSE results to compare various models
rmse_results <- data_frame()
```

### Sample estimate- mean
The initial step is to compute the dataset's mean rating.

```{r mean_calc}
mu <- mean(edx$rating)  
mu
```

### Movie Effect - Penalty 

Popular movies have higher rating mostly and unpopular movies have low ratings. The histogram is left skewed and it shows that more movies have negative effects.

``` {r movie_effect}
movie_av <- edx %>% 
    group_by(movieId) %>% 
    summarize(b_i = mean(rating - mu))

movie_av %>% qplot(b_i, geom ="histogram", bins = 20, data = ., color = I("red"))
  
```

### User Effect - Penalty 

Some users can also affect the ratings either positively(by giving higher ratings) or negatively.

``` {r user_effect}
user_av <- edx %>% 
    left_join(movie_av, by='movieId') %>%
    group_by(userId) %>%
    summarize(b_u = mean(rating - mu - b_i))

user_av %>% qplot(b_u, geom ="histogram", bins = 30, data = ., color = I("green"))
  
```
## Model Creation
#### Model Validation
  
The quality of the model will be assessed by the RMSE (the lower the better).

### Naive Model : just the mean

This model uses the sample mean represents the initial simplest model. This implies that prediction is the sample average. The resulting RMSE is quite high with this model.

```{r mean only}
# Naive Model -- mean only
naive_rmse <- RMSE(validation_CM$rating,mu)
## Test results based on simple prediction
naive_rmse
## Check results
rmse_results <- data_frame(method = "Using mean only", RMSE = naive_rmse)
rmse_results
## Save prediction in data frame
```
### Movie Effect Model

The RMSE improvisation can be done by adding the movie effect.

```{r movie_effect_model}
predrat_movie_norm <- validation %>% 
    left_join(movie_av, by='movieId') %>%
    mutate(pred = mu + b_i) 

model_1_rmse <- RMSE(validation_CM$rating,predrat_movie_norm$pred)

rmse_results <- bind_rows(rmse_results,
                            data_frame(method="Movie Effect Model",  
                                       RMSE = model_1_rmse ))
rmse_results %>% knitr::kable()
rmse_results
```

### Movie and User Effect Model

Next improvisation is achieved by adding the user effect.

```{r user_movie_model}
# Use test set,join movie averages & user averages
# Prediction equals the mean with user effect b_u & movie effect b_i
  predrat_user_norm <- validation %>% 
    left_join(movie_av, by='movieId') %>%
    left_join(user_av, by='userId') %>%
    mutate(pred = mu + b_i + b_u) 

  # test and save rmse results 
  
  model_2_rmse <- RMSE(validation_CM$rating,predrat_user_norm$pred)
  rmse_results <- bind_rows(rmse_results,
                            data_frame(method="Movie and User Effect Model",  
                                       RMSE = model_2_rmse ))
  rmse_results %>% knitr::kable()
  rmse_results

```

### Regularized Movie and User Effect Model 

This model adds the concept of regularization to account for the effect of low ratings' numbers for movies and users. This regularization used to reduce the effect of overfitting. 

```{r regularized movie and user model}
# lambda is a tuning parameter
# Use cross-validation to choose it.
lambdas <- seq(0, 10, 0.25)
  # For each lambda,find b_i & b_u, followed by rating prediction & testing
  # note:the below code could take some time  
  rmses <- sapply(lambdas, function(l){
    
    mu <- mean(edx$rating)
    
    b_i <- edx %>% 
      group_by(movieId) %>%
      summarize(b_i = sum(rating - mu)/(n()+l))
    
    b_u <- edx %>% 
      left_join(b_i, by="movieId") %>%
      group_by(userId) %>%
      summarize(b_u = sum(rating - b_i - mu)/(n()+l))
    
    predrat <- 
      validation %>% 
      left_join(b_i, by = "movieId") %>%
      left_join(b_u, by = "userId") %>%
      mutate(pred = mu + b_i + b_u) %>%
      .$pred
      return(RMSE(predrat,validation_CM$rating))

  })
  
  # Plot rmses vs lambdas to select the optimal lambda                                                             
  qplot(lambdas, rmses)  
  
  # The optimal lambda                                                             
  lambda <- lambdas[which.min(rmses)]
  
  # Compute regularized estimates of b_i using lambda
  movie_av_reg <- edx %>% 
    group_by(movieId) %>% 
    summarize(b_i = sum(rating - mu)/(n()+lambda), n_i = n())
  
  # Compute regularized estimates of b_u using lambda
  
  user_av_reg <- edx %>% 
    left_join(movie_av_reg, by='movieId') %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - mu - b_i)/(n()+lambda), n_u = n())
  
  # Predict ratings
  
  predrat_reg <- validation %>% 
    left_join(movie_av_reg, by='movieId') %>%
    left_join(user_av_reg, by='userId') %>%
    mutate(pred = mu + b_i + b_u) %>% 
    .$pred
  
  
  # Test and save results
  
  model_3_rmse <- RMSE(validation_CM$rating,predrat_reg)
  rmse_results <- bind_rows(rmse_results,
                            data_frame(method="Regularized Movie and User Effect Model",
                                       RMSE = model_3_rmse ))
  rmse_results %>% knitr::kable()
  rmse_results

```

### Regularized Movie, User and year Effect Model

This model adds the concept of regularization to account for the effect of low ratings' numbers for year in-addition to the movie and user. This regularization used to reduce the effect of overfitting compared to earlier model. 

```{r regularized movie, user and year model}
# lambda is a tuning parameter
# Use cross-validation to choose it.
lambdas <- seq(0, 10, 0.25)
  
# For each lambda,find b_i1, b_u1 & b_y1, followed by rating prediction & testing
  
  rmses <- sapply(lambdas, function(l){
    
    mu <- mean(edx$rating)
    
    b_i1 <- edx %>% 
      group_by(movieId) %>%
      summarize(b_i1 = sum(rating - mu)/(n()+l))
    
    b_u1 <- edx %>% 
      left_join(b_i1, by="movieId") %>%
      group_by(userId) %>%
      summarize(b_u1 = sum(rating - b_i1 - mu)/(n()+l))
    
    b_y1 <- edx %>% 
      left_join(b_i1, by="movieId") %>%
      left_join(b_u1, by="userId") %>%
      group_by(year) %>%
      summarize(b_y1 = sum(rating - b_u1 - b_i1 - mu)/(n()+l))

    predrat1 <- 
      validation %>% 
      left_join(b_i1, by = "movieId") %>%
      left_join(b_u1, by = "userId") %>%
      left_join(b_y1, by = "year") %>%
      mutate(pred = mu + b_i1 + b_u1 + b_y1) %>%
      pull(pred)
      
      return(RMSE(predrat1, validation_CM$rating))
  })

  # Plot rmses vs lambdas to select the optimal lambda                                                             
  qplot(lambdas, rmses)  

  # The optimal lambda                                                             
  lambda <- lambdas[which.min(rmses)]
  
  # Compute regularized estimates of b_i1 using lambda
  movie_av_reg1 <- edx %>% 
    group_by(movieId) %>% 
    summarize(b_i1 = sum(rating - mu)/(n()+lambda), n_i1 = n())
  
  # Compute regularized estimates of b_u1 using lambda
  
  user_av_reg1 <- edx %>% 
    left_join(movie_av_reg1, by='movieId') %>%
    group_by(userId) %>%
    summarize(b_u1 = sum(rating - mu - b_i1)/(n()+lambda), n_u1 = n())
  
  # Compute regularized estimates of b_y1 using lambda
  
  year_av_reg1 <- edx %>% 
    left_join(movie_av_reg1, by='movieId') %>%
    left_join(user_av_reg1, by='userId') %>%
    group_by(year) %>%
    summarize(b_y1 = sum(rating - mu - b_i1 - b_u1)/(n()+lambda), n_y1 = n())
  
  # Predict ratings
  
  predrat_reg1 <- validation %>% 
    left_join(movie_av_reg1, by='movieId') %>%
    left_join(user_av_reg1, by='userId') %>%
    left_join(year_av_reg1, by = 'year') %>%
    mutate(pred = mu + b_i1 + b_u1 + b_y1) %>% 
    .$pred
  
  
  # Test and save results
  
  model_4_rmse <- RMSE(validation_CM$rating,predrat_reg1)
  rmse_results <- bind_rows(rmse_results,
                            data_frame(method="Regularized Movie User and Year Effect Model",
                                       RMSE = model_4_rmse ))
  rmse_results %>% knitr::kable()
  rmse_results

```

# Results
The RMSE values of all the represented models are the following:

```{r final results}
rmse_results
```

As per above details, we found the lowest value of RMSE is 0.8647687.

# Conclusion

We have built a machine learning algorithm with different models to predict movie ratings. The regularised model including the effect of movie, user and year is characterized by the lower RMSE value. Hence, this is the optimal model for this project. This model RMSE value is 0.8647687 which is lower than the evaluation criteria (0.86490). We can also improve the RMSE by adding other effects like genre and age.

\pagebreak

# Appendix - Enviroment

```{r}
print("Operating System:")
version
```